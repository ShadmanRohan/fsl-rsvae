{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP7OnpougYFWbivA5s/O8Hv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShadmanRohan/fsl-rsvae/blob/main/starter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Fkr40LMo7uc",
        "outputId": "5e01e622-6b8e-422e-fab7-33cc9eb0cf58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'fsl-rsvae'...\n",
            "remote: Enumerating objects: 401, done.\u001b[K\n",
            "remote: Counting objects: 100% (401/401), done.\u001b[K\n",
            "remote: Compressing objects: 100% (210/210), done.\u001b[K\n",
            "remote: Total 401 (delta 192), reused 394 (delta 190), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (401/401), 426.44 KiB | 1.21 MiB/s, done.\n",
            "Resolving deltas: 100% (192/192), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ShadmanRohan/fsl-rsvae.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "4ktupjgODfja",
        "outputId": "4fa14883-7c7d-4fa1-b0aa-6f46d237164e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from distutils.dir_util import copy_tree\n",
        "copy_tree(\"/content/gdrive/MyDrive/PAMI/AWA1_AWA2_SUN/data/AWA1\", \"/content/fsl-rsvae/datasets/AWA1\")"
      ],
      "metadata": {
        "id": "IEef9zeLD52b",
        "outputId": "1e09a56c-0b39-470f-c7c2-31e07221e87e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/fsl-rsvae/datasets/AWA1/seen_test.mat',\n",
              " '/content/fsl-rsvae/datasets/AWA1/seen_attribute.mat',\n",
              " '/content/fsl-rsvae/datasets/AWA1/seen_test_label.mat',\n",
              " '/content/fsl-rsvae/datasets/AWA1/seen_train.mat',\n",
              " '/content/fsl-rsvae/datasets/AWA1/unseen_attribute.mat',\n",
              " '/content/fsl-rsvae/datasets/AWA1/seen_train_label.mat',\n",
              " '/content/fsl-rsvae/datasets/AWA1/unseen_test.mat',\n",
              " '/content/fsl-rsvae/datasets/AWA1/unseen_test_label.mat']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import argparse\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "from torchvision.datasets.folder import DatasetFolder\n",
        "from torch.distributions import uniform, normal\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from matplotlib import pyplot as plt\n",
        "import torch.optim\n",
        "import json\n",
        "import torch.utils.data.sampler\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import time\n",
        "import pdb\n",
        "import yaml\n",
        "#import datasets.feature_loader as feat_loader\n",
        "from sklearn.manifold import TSNE\n",
        "import h5py\n",
        "from scipy.stats import multivariate_normal\n",
        "import scipy"
      ],
      "metadata": {
        "id": "k8kzzG1xpNSE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def finetune_vae(feats_vae, x_shot, label_real):\n",
        "    attributes = np.load('./mini_attr.npy')\n",
        "    x_shot = x_shot.detach()\n",
        "    z_dist = normal.Normal(0, 1)\n",
        "    bs_list = np.arange(4)\n",
        "    feats_vae.train()\n",
        "    optimizer = torch.optim.Adam(feats_vae.parameters(), lr=0.0001)\n",
        "    for ep in range(5):\n",
        "      np.random.shuffle(bs_list)\n",
        "      for idx in bs_list:\n",
        "        targets = x_shot[idx]\n",
        "        labels_sel = label_real[idx] + 80\n",
        "        attr = torch.from_numpy(attributes[labels_sel]).float().cuda()\n",
        "        attr = attr.repeat((1, 50)).reshape((5, 50, -1))\n",
        "        Z = z_dist.sample((5, 50, 512)).cuda()\n",
        "        concat_feats = torch.cat((Z, attr), dim=2)\n",
        "        concat_feats = torch.autograd.Variable(concat_feats, requires_grad=True)\n",
        "        feats = feats_vae.model(concat_feats).reshape((-1, 512))\n",
        "        feats = feats_vae.relu(feats_vae.bn1(feats)).reshape((5, 50, 512))\n",
        "        feats = feats.mean(1)\n",
        "        feats = F.normalize(feats, dim=-1)\n",
        "        mse_loss = F.mse_loss(feats, targets)\n",
        "        optimizer.zero_grad()\n",
        "        mse_loss.backward()\n",
        "        optimizer.step()\n",
        "        print(mse_loss.item())"
      ],
      "metadata": {
        "id": "diZ7mTLrpE2Y"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def shrink_feats(cl_data_file):\n",
        "    cl_mean_file = {}\n",
        "    weight_data_file = {}\n",
        "    for k, v in cl_data_file.items():\n",
        "        mean_feats = np.mean(v, 0)\n",
        "        cl_mean_file[k] = mean_feats / np.sqrt(np.sum(mean_feats*mean_feats))\n",
        "    for k, v in cl_data_file.items():\n",
        "        v = np.array(v)\n",
        "        v = v / np.sqrt(np.sum(v*v, -1, keepdims=True))\n",
        "        dist = np.sum((v - cl_mean_file[k])**2, 1)\n",
        "        sort_idx = np.argsort(dist)\n",
        "        in_idx = sort_idx[:50]\n",
        "        out_idx = sort_idx[50:200]\n",
        "        in_feats = v[in_idx]\n",
        "        out_feats = v[out_idx]\n",
        "        cl_data_file[k] = []\n",
        "        for in_f in in_feats:\n",
        "          cl_data_file[k].append(in_f)\n",
        "        for o_idx in out_idx:\n",
        "          close_feats = (v[o_idx] + cl_mean_file[k]) / 2\n",
        "          close_dist = np.sum((in_feats - close_feats)**2, -1)\n",
        "          min_idx = np.argsort(close_dist)[0]\n",
        "          cl_data_file[k].append(in_feats[min_idx])\n",
        "    pdb.set_trace()\n",
        "    return cl_data_file\n",
        "\n",
        "def det(matrix):\n",
        "    order=len(matrix)\n",
        "    posdet=0\n",
        "    for i in range(order):\n",
        "        posdet+=reduce((lambda x, y: x * y), [matrix[(i+j)%order][j] for j in range(order)])\n",
        "    negdet=0\n",
        "    for i in range(order):\n",
        "        negdet+=reduce((lambda x, y: x * y), [matrix[(order-i-j)%order][j] for j in range(order)])\n",
        "    return posdet-negdet\n",
        "\n",
        "\n",
        "def remove_feats(cl_data_file):\n",
        "    cl_mean_file = {}\n",
        "    cl_var_file = {}\n",
        "    weight_data_file = {}\n",
        "    remove_num = []\n",
        "    for k, v in cl_data_file.items():\n",
        "        mean_feats = np.mean(v, 0)\n",
        "        cl_mean_file[k] = mean_feats\n",
        "        cl_var_file[k] = np.cov(np.array(v).T)\n",
        "    for k, v in cl_data_file.items():\n",
        "        v = np.array(v)\n",
        "        #dist = np.sum((v - cl_mean_file[k])**2, 1)\n",
        "        #sort_idx = np.argsort(dist)[:220]\n",
        "        cl_data_file[k] = []\n",
        "        #weight_data_file[k] = []\n",
        "        inv_var = scipy.linalg.inv(cl_var_file[k])\n",
        "        mean = cl_mean_file[k]\n",
        "        prob = np.sum(np.matmul((v-mean),inv_var)*(v-mean), -1)\n",
        "        prob = 1-scipy.stats.chi2.cdf(prob, 512)\n",
        "        #rv = multivariate_normal(mean = cl_mean_file[k], cov = cl_var_file[k])\n",
        "        for idx in range(600):\n",
        "          if prob[idx] > 0.9:\n",
        "            cl_data_file[k].append(v[idx]) \n",
        "        remove_num.append(np.sum(prob<0.9))\n",
        "        #for sidx in sort_idx:\n",
        "        #  cl_data_file[k].append(v[sidx])\n",
        "        #  cl_data_file[k].append((cl_mean_file[k] + (np.random.normal(v[sidx].shape)*0.001)).astype(np.float32))\n",
        "        #  weight_data_file[k].append(1./dist[sidx])\n",
        "        #all_feats = (np.random.multivariate_normal(mean=cl_mean_file[k], cov=cl_var_file[k], size=200)).astype(np.float32)\n",
        "        #for all_feat in all_feats:\n",
        "        #  cl_data_file[k].append(all_feat*0.3 + cl_mean_file[k]*0.7)\n",
        "    pdb.set_trace()\n",
        "    return cl_data_file\n",
        "\n",
        "def interpolate_feats(cl_data_file):\n",
        "    cl_mean_file = {}\n",
        "    for k, v in cl_data_file.items():\n",
        "        mean_feats = np.mean(v, 0)\n",
        "        cl_mean_file[k] = mean_feats\n",
        "    for k, v in cl_data_file.items():\n",
        "        v = np.array(v) \n",
        "        dist = np.sum((v - cl_mean_file[k])**2, 1)\n",
        "        cl_data_file[k] = []\n",
        "        for iv in v:\n",
        "          cl_data_file[k].append(0.6*iv+0.4*cl_mean_file[k])\n",
        "\n",
        "    return cl_data_file\n",
        "\n",
        "def get_vae_center(out_dir, split='train', use_mean=True):\n",
        "    attr_out_file = os.path.join(out_dir, '%s_attr.hdf5'%split)\n",
        "    vae_data_file = feat_loader.init_loader(attr_out_file)\n",
        "    if 'train' in split:\n",
        "      num = 64\n",
        "    else:\n",
        "      num = 20\n",
        "    if use_mean:\n",
        "      vae_feats_all = torch.zeros((num, 512))\n",
        "    else:\n",
        "      vae_feats_all = torch.zeros((num, 20, 512))\n",
        "    mean_data_file = {}\n",
        "\n",
        "    for k, feats in vae_data_file.items():\n",
        "        mean_feats = np.mean(feats, 0)\n",
        "        mean_data_file[k] = mean_feats\n",
        "\n",
        "    for k, feats in vae_data_file.items():\n",
        "        #mean_feats = np.array(feats)[:50]\n",
        "        #mean_feats = 3*mean_feats - 2*mean_data_file[k]\n",
        "        if use_mean:\n",
        "          mean_feats = np.mean(feats, 0)\n",
        "        else:\n",
        "          mean_feats = np.array(feats)[:20]\n",
        "        mean_feats = torch.from_numpy(mean_feats)\n",
        "        mean_feats = F.normalize(mean_feats, dim=-1) \n",
        "        if 'test' in split: \n",
        "          k = k - 80\n",
        "        vae_feats_all[k] = mean_feats\n",
        "  \n",
        "    return vae_feats_all\n",
        "\n",
        "\n",
        "def generate_feats(feats_vae, attributes, output_file, label_list):\n",
        "    f = h5py.File(output_file, 'w')\n",
        "    ind_count = 500\n",
        "    max_count = ind_count * len(label_list)\n",
        "    all_labels = f.create_dataset('all_labels',(max_count,), dtype='i')\n",
        "    all_feats=None\n",
        "    count=0\n",
        "    feats_vae.eval()\n",
        "    z_dist = normal.Normal(0, 1)\n",
        "    for label in label_list:\n",
        "        attr = torch.from_numpy(attributes[label]).float().cuda()\n",
        "        attr = attr.repeat(ind_count, 1)\n",
        "        Z = z_dist.sample((ind_count, 512)).cuda()\n",
        "        concat_feats = torch.cat((Z, attr), dim=1)\n",
        "        feats = feats_vae.model(concat_feats)\n",
        "        feats = feats_vae.relu(feats_vae.bn1(feats))\n",
        "        if all_feats is None:      \n",
        "          all_feats = f.create_dataset('all_feats', [max_count] + list(feats.size()[1:]) , dtype='f')\n",
        "        all_feats[count:count+feats.size(0)] = feats.data.cpu().numpy()\n",
        "        all_labels[count:count+feats.size(0)] = np.array([label]*ind_count)\n",
        "        count = count + feats.size(0)\n",
        "    count_var = f.create_dataset('count', (1,), dtype='i')\n",
        "    count_var[0] = count\n",
        "\n",
        "    f.close()\n",
        "\n",
        "       \n",
        "\n",
        "def train_vae(feature_loader, feats_vae, attributes):\n",
        "    optimizer = torch.optim.Adam(feats_vae.parameters(), lr=0.001)\n",
        "    #for ep in range(10):\n",
        "    for ep in range(60):\n",
        "      loss_recon_all = 0\n",
        "      loss_kl_all = 0\n",
        "      for idx, (data, label) in enumerate(feature_loader):\n",
        "        data = data.cuda()\n",
        "        #weight = weight.cuda() / torch.sum(weight)\n",
        "        attr = torch.from_numpy(attributes[label]).float().cuda()\n",
        "        mu, logvar, recon_feats = feats_vae(data, attr)\n",
        "        recon_loss = ((recon_feats - data)**2).mean(1)\n",
        "        recon_loss = torch.mean(recon_loss)\n",
        "        #kl_loss = -0.5*torch.sum(1+logvar-logvar.exp()-mu.pow(2)) / data.shape[0]\n",
        "        kl_loss = (1+logvar-logvar.exp()-mu.pow(2)).sum(1)\n",
        "        kl_loss = -0.5*torch.mean(kl_loss)\n",
        "        L_vae = recon_loss+kl_loss*0.005\n",
        "        optimizer.zero_grad()\n",
        "        L_vae.backward()   \n",
        "        optimizer.step()\n",
        "        loss_recon_all += recon_loss.item()\n",
        "        loss_kl_all += kl_loss.item()\n",
        "      print('Ep: %d   Recon Loss: %f   KL Loss: %f'%(ep, loss_recon_all/(idx+1), loss_kl_all/(idx+1)))\n",
        "    return feats_vae\n",
        "    #torch.save({'state': feats_vae.state_dict()}, 'feats_vae_mini.pth') \n",
        "\n",
        "\n",
        "def visualize_feats(feats_dir):\n",
        "    visual_feats = []\n",
        "    attr_feats = []\n",
        "    visual_labels = []\n",
        "    attr_labels = []\n",
        "    cl_data_file = os.path.join(feats_dir, 'test.hdf5')\n",
        "    cl_data_file = feat_loader.init_loader(cl_data_file)\n",
        "    vae_data_file = os.path.join(feats_dir, 'test_attr_ood.hdf5')\n",
        "    vae_data_file = feat_loader.init_loader(vae_data_file)\n",
        "    pdb.set_trace()\n",
        "    #labels = [51, 3, 179, 7, 11, 175] \n",
        "    #labels = [15,6,17,8,9]\n",
        "    labels = [85, 86, 87, 88, 89]\n",
        "    #labels = [13, 17, 21, 29, 33, 37]\n",
        "    tsne = TSNE(n_components=2, random_state=0)\n",
        "    for idx in range(5):\n",
        "        label = labels[idx]\n",
        "        visual_feats.extend(cl_data_file[label-80][:100])\n",
        "        #attr_feats.extend((vae_data_file[label][:300]))\n",
        "        #attr_feats.extend(np.mean(np.array(vae_data_file[label]), 0, keepdims=True))\n",
        "        visual_labels.extend([idx]*len(cl_data_file[label-80][:100]))\n",
        "        #attr_labels.extend([idx]*len(vae_data_file[label][:300]))\n",
        "        #attr_labels.extend([idx])\n",
        "    visual_feats = np.array(visual_feats)\n",
        "    #attr_feats = np.array(attr_feats)\n",
        "    #all_feats = np.concatenate((visual_feats, attr_feats), 0)\n",
        "    pdb.set_trace()\n",
        "    all_labels = visual_labels \n",
        "    all_feats = visual_feats\n",
        "    all_feats_2D = tsne.fit_transform(all_feats)\n",
        "    #all_feats_2D = tsne.fit_transform(visual_feats)\n",
        "    #all_labels = visual_labels\n",
        "    colors = np.array(['r', 'g', 'b', 'c', 'm', 'y', 'k',  'orange', 'purple'])      \n",
        "    for idx in range(all_feats_2D.shape[0]):\n",
        "        feat = all_feats_2D[idx]\n",
        "        #if feat[0] < -30 or feat[1] < -30:\n",
        "        #  continue\n",
        "        label = all_labels[idx]\n",
        "        color = colors[label]\n",
        "        if idx < visual_feats.shape[0]:\n",
        "          marker = '*'\n",
        "          #continue\n",
        "        else:\n",
        "          marker = 'o'\n",
        "          #continue\n",
        "        plt.scatter(feat[0], feat[1], c=color, marker=marker) \n",
        "    plt.savefig('features_base_mini.png')\n",
        "\n",
        "def save_vae_features(out_file, attr_out_dir):\n",
        "    cl_data_file = feat_loader.init_loader(out_file)\n",
        "    cl_data_file = remove_feats(cl_data_file)\n",
        "    feature_dataset = FeatureDataset(cl_data_file)\n",
        "    feature_loader = torch.utils.data.DataLoader(feature_dataset, shuffle=True, pin_memory=True, drop_last=False, batch_size=256) \n",
        "    attributes = np.load('./mini_attr.npy')\n",
        "    feats_vae = FeatsVAE(512, 512).cuda()\n",
        "    feats_vae = train_vae(feature_loader, feats_vae, attributes)\n",
        "    #feats_vae.load_state_dict(torch.load('feats_vae_mini.pth')['state'])\n",
        "    #torch.save({'state': feats_vae.state_dict()}, 'feats_vae_mini.pth') \n",
        "    generate_feats(feats_vae, attributes, os.path.join(attr_out_dir, 'train_attr.hdf5'), np.arange(0, 64))\n",
        "    generate_feats(feats_vae, attributes, os.path.join(attr_out_dir, 'test_attr.hdf5'), np.arange(80, 100))\n",
        "    #return feats_vae\n",
        "\n"
      ],
      "metadata": {
        "id": "xn3pYo2fpYek"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#/content/fsl-rsvae/configs/test_few_shot.yaml\n",
        "config = yaml.load(open(args.config, 'r'), Loader=yaml.FullLoader)\n",
        "out_dir = os.path.dirname(config['load_encoder'])\n",
        "out_file = os.path.join(out_dir, 'features', 'test.hdf5')\n",
        "cl_data_file = feat_loader.init_loader(out_file)\n",
        "feature_dataset = FeatureDataset(cl_data_file)\n",
        "feature_loader = torch.utils.data.DataLoader(feature_dataset, shuffle=True, pin_memory=True, drop_last=False, batch_size=256)\n",
        "     \n",
        "#attributes = np.load('./mini_attr.npy')\n",
        "feats_vae = FeatsVAE(512, 512).cuda()\n",
        "train_vae(feature_loader, feats_vae, attributes)\n",
        "#generate_feats(feats_vae, attributes, os.path.join(out_dir, 'features', 'test_attr.hdf5'), np.arange(80, 100))\n",
        "#save_vae_features(out_file, out_dir)\n",
        "\n",
        "#vae_feats_file = os.path.join(out_dir, 'features', 'test_attr.hdf5')\n",
        "#vae_data_file = feat_loader.init_loader(vae_feats_file)\n",
        "#visualize_feats(cl_data_file, vae_data_file)\n"
      ],
      "metadata": {
        "id": "TcagUsiDDFXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.io\n",
        "#features\n",
        "data_train = scipy.io.loadmat('/content/fsl-rsvae/datasets/AWA1/seen_train.mat')\n",
        "data_train = data_train['feature']\n",
        "#data_train = np.transpose(data_train)\n",
        "\n",
        "\n",
        "# labels\n",
        "label_train = scipy.io.loadmat('/content/fsl-rsvae/datasets/AWA1/seen_train_label.mat')\n",
        "data_train_label = label_train['label'][0]\n",
        "\n",
        "\n",
        "# attr\n",
        "tmp = scipy.io.loadmat('/content/fsl-rsvae/datasets/AWA1/seen_attribute.mat')\n",
        "attr = tmp['seen_attribute']\n",
        "#attr = np.transpose(attr)"
      ],
      "metadata": {
        "id": "KWmLvTptFmf9"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureDataset(DatasetFolder):\n",
        "\n",
        "    def __init__(self, features, labels, attr):\n",
        "        self.labels = labels\n",
        "        self.features = features\n",
        "        self.attr = attr\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(data_train)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.labels[idx]\n",
        "\n",
        "feature_dataset = FeatureDataset(data_train, data_train_label, attr)\n",
        "feature_loader = torch.utils.data.DataLoader(feature_dataset, shuffle=True, pin_memory=True, drop_last=False, batch_size=256) "
      ],
      "metadata": {
        "id": "OiH7f_bqFb3i"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatsVAE(nn.Module):\n",
        "    def __init__(self, x_dim, latent_dim):\n",
        "        super(FeatsVAE, self).__init__()\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(x_dim+latent_dim, 4096),\n",
        "            #nn.LeakyReLU(),\n",
        "            #nn.Linear(4096, 4096),\n",
        "            nn.LeakyReLU())\n",
        "        self.linear_mu =  nn.Sequential(\n",
        "            nn.Linear(4096, latent_dim),\n",
        "            nn.ReLU())\n",
        "        self.linear_logvar =  nn.Sequential(\n",
        "            nn.Linear(4096, latent_dim),\n",
        "            nn.ReLU())\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(2*latent_dim, 4096),\n",
        "            nn.LeakyReLU(),\n",
        "            #nn.Linear(4096, 4096),\n",
        "            #nn.LeakyReLU(),\n",
        "            nn.Linear(4096, x_dim),\n",
        "            #nn.Sigmoid(),\n",
        "        )\n",
        "        self.bn1 = nn.BatchNorm1d(x_dim)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.z_dist = normal.Normal(0, 1)\n",
        "        self.init_weights()\n",
        "\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5*logvar)  \n",
        "        eps = torch.randn_like(std)\n",
        "        # remove abnormal points\n",
        "        return mu + eps*std\n",
        "\n",
        "    def init_weights(self):\n",
        "        for m in self.modules():\n",
        "          if isinstance(m, nn.Linear):\n",
        "              m.weight.data.normal_(0, 0.02)\n",
        "              m.bias.data.normal_(0, 0.02)\n",
        "\n",
        "    def forward(self, x, attr):\n",
        "        x = torch.cat((x, attr), dim=1)\n",
        "        x = self.linear(x)\n",
        "        mu = self.linear_mu(x)\n",
        "        logvar = self.linear_logvar(x)\n",
        "        latent_feats = self.reparameterize(mu, logvar)\n",
        "        #Z = self.z_dist.sample(attr.shape).cuda() \n",
        "        concat_feats = torch.cat((latent_feats, attr), dim=1)\n",
        "        recon_feats = self.model(concat_feats)\n",
        "        recon_feats = self.relu(self.bn1(recon_feats))\n",
        "        return mu, logvar, recon_feats\n",
        "\n",
        "feats_vae = FeatsVAE(512, 512)"
      ],
      "metadata": {
        "id": "QR8I-FHnK1gq"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_vae(feature_loader, feats_vae, attributes):\n",
        "    optimizer = torch.optim.Adam(feats_vae.parameters(), lr=0.001)\n",
        "    #for ep in range(10):\n",
        "    for ep in range(60):\n",
        "      loss_recon_all = 0\n",
        "      loss_kl_all = 0\n",
        "      for idx, (data, label) in enumerate(feature_loader):\n",
        "        print(\"training loop...\")\n",
        "        data = data\n",
        "        #weight = weight.cuda() / torch.sum(weight)\n",
        "        attr = torch.from_numpy(attributes[label]).float()\n",
        "        mu, logvar, recon_feats = feats_vae(data, attr)\n",
        "        recon_loss = ((recon_feats - data)**2).mean(1)\n",
        "        recon_loss = torch.mean(recon_loss)\n",
        "        #kl_loss = -0.5*torch.sum(1+logvar-logvar.exp()-mu.pow(2)) / data.shape[0]\n",
        "        kl_loss = (1+logvar-logvar.exp()-mu.pow(2)).sum(1)\n",
        "        kl_loss = -0.5*torch.mean(kl_loss)\n",
        "        L_vae = recon_loss+kl_loss*0.005\n",
        "        optimizer.zero_grad()\n",
        "        L_vae.backward()   \n",
        "        optimizer.step()\n",
        "        loss_recon_all += recon_loss.item()\n",
        "        loss_kl_all += kl_loss.item()\n",
        "      print('Ep: %d   Recon Loss: %f   KL Loss: %f'%(ep, loss_recon_all/(idx+1), loss_kl_all/(idx+1)))\n",
        "    return feats_vae\n",
        "    #torch.save({'state': feats_vae.state_dict()}, 'feats_vae_mini.pth') \n",
        "\n",
        "train_vae(feature_loader, feats_vae, attr)"
      ],
      "metadata": {
        "id": "JN9RRAsJNhzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nHyD_-z0ONRN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}